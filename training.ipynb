{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_higher_images = '/mnt/f/datasets/SR_training_datasets/DIV2K/DIV2K_train_HR.zip'\n",
    "train_lower_images = '/mnt/f/datasets/SR_training_datasets/DIV2K/DIV2K_train_LR_bicubic_X4.zip'\n",
    "\n",
    "valid_higher_images = '/mnt/f/datasets/SR_training_datasets/DIV2K/DIV2K_valid_HR.zip'\n",
    "valid_lower_images = '/mnt/f/datasets/SR_training_datasets/DIV2K/DIV2K_valid_LR_bicubic_X4.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZipDataset(Dataset):\n",
    "    def __init__(self, zip_file_HR, zip_file_LR, transform=None):\n",
    "        self.zip_file_HR = ZipFile(zip_file_HR)\n",
    "        self.zip_file_LR = ZipFile(zip_file_LR)\n",
    "        self.file_list_HR = self.zip_file_HR.namelist()\n",
    "        self.file_list_HR = [file for file in self.file_list_HR if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.file_list_LR = self.zip_file_LR.namelist()\n",
    "        self.file_list_LR = [file for file in self.file_list_LR if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = get_transform()\n",
    "\n",
    "        self.lr_mapping = {os.path.splitext(os.path.basename(name))[0].split('x')[0]: i for i, name in enumerate(self.file_list_LR)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list_HR)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_HR = self.file_list_HR[idx]\n",
    "        # Get the base name of the HR image\n",
    "        base_name_HR = os.path.splitext(os.path.basename(file_HR))[0]\n",
    "        # Find the corresponding LR image\n",
    "        idx_LR = self.lr_mapping[base_name_HR]\n",
    "        file_LR = self.file_list_LR[idx_LR]\n",
    "        \n",
    "        # printing to check the image names are same or not\n",
    "        \n",
    "        # print(file_HR)\n",
    "        # print(file_LR)\n",
    "\n",
    "        with self.zip_file_HR.open(file_HR) as f:\n",
    "            image_HR = Image.open(BytesIO(f.read()))\n",
    "        with self.zip_file_LR.open(file_LR) as f:\n",
    "            image_LR = Image.open(BytesIO(f.read()))\n",
    "        if self.transform:\n",
    "            image_HR = self.transform(image_HR)\n",
    "            image_LR = self.transform(image_LR)\n",
    "\n",
    "        return image_LR, image_HR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ZipDataset(train_higher_images, train_lower_images)\n",
    "valid_dataset = ZipDataset(valid_higher_images, valid_lower_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    return [data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=my_collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the dataloader\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    # print the shapes of the images and labels\n",
    "    print(f'Batch {i+1}:')\n",
    "    print('Images shape:', images[0].shape)\n",
    "    print('Labels shape:', labels[0].shape)\n",
    "    # break the loop after the first batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    # Select the first image from the batch\n",
    "    # \n",
    "    tensor = tensor.cuda()\n",
    "\n",
    "    # Define the mean and std\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "    # Unnormalize the tensor\n",
    "    std = std.cuda()\n",
    "    mean = mean.cuda()\n",
    "    tensor = tensor * std + mean\n",
    "\n",
    "    # Clamp the values in the tensor to the range [0, 1]\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "\n",
    "    # Convert the tensor to a PIL Image and then convert it to a numpy array\n",
    "    image = transforms.ToPILImage()(tensor).convert(\"RGB\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RCAT module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCat(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super(RCat, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(f, f//4, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(f, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv5 = nn.Conv2d(16, 16, kernel_size=7, stride=1, padding=3)\n",
    "        self.conv6 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(f//2, f//4, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = F.relu(self.conv1(x))\n",
    "        x1 = F.relu(self.conv2(x))\n",
    "        x1 = F.relu(self.conv3(x1))\n",
    "        x1 = F.relu(self.conv4(x1))\n",
    "        x1 = F.relu(self.conv5(x1))\n",
    "        x1 = F.relu(self.conv6(x1))\n",
    "        c1 = torch.cat((x1, y1), dim=1)\n",
    "        c2 = F.relu(self.conv7(c1))\n",
    "        return c2 + y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDN(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super(RDN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(f, f, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(f, f, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(f, f, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(f, f, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(3*f, f//4, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.conv1(x)\n",
    "        y2 = self.conv2(y1)\n",
    "        a1 = y1 + y2\n",
    "        y3 = self.conv3(a1)\n",
    "        a2 = y3 + a1\n",
    "        y4 = self.conv4(a2)\n",
    "        a3 = a1 + a2 + y4\n",
    "        c = torch.cat((a1, a2, a3), dim=1)\n",
    "        return self.conv5(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUNT module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runt(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super(Runt, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(f, f, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(f, f, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(f, f//2, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(f//2, f//2, kernel_size=5, padding=2)\n",
    "        self.conv5 = nn.Conv2d(f//2, f//4, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(f//4, f//4, kernel_size=5, padding=2)\n",
    "        self.conv7 = nn.Conv2d(f//4, f//8, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(f//8, f//8, kernel_size=5, padding=2)\n",
    "        self.conv9 = nn.Conv2d(f//8, f//8, kernel_size=1)\n",
    "        self.conv10 = nn.Conv2d(f//4, f//4, kernel_size=1)\n",
    "        self.conv11 = nn.Conv2d(f//2, f//2, kernel_size=1)\n",
    "        self.conv12 = nn.Conv2d(f, f, kernel_size=1)\n",
    "        self.conv13 = nn.Conv2d(f, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = F.relu(self.conv1(x))\n",
    "        y1 = F.relu(self.conv2(y1))\n",
    "        y2 = F.relu(self.conv3(y1))\n",
    "        y2 = F.relu(self.conv4(y2))\n",
    "        y3 = F.relu(self.conv5(y2))\n",
    "        y3 = F.relu(self.conv6(y3))\n",
    "        y4 = F.relu(self.conv7(y3))\n",
    "        y4 = F.relu(self.conv8(y4))\n",
    "        y5 = F.relu(self.conv9(y4))\n",
    "        c1 = torch.cat([y5, y4], dim=1)\n",
    "        y6 = F.relu(self.conv10(c1))\n",
    "        c2 = torch.cat([y6, y3], dim=1)\n",
    "        y7 = F.relu(self.conv11(c2))\n",
    "        c3 = torch.cat([y7, y2], dim=1)\n",
    "        y8 = F.relu(self.conv12(c3))\n",
    "        y8 = y8 + y1\n",
    "        y9 = self.conv13(y8)\n",
    "        return y9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Den(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Den, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=1)\n",
    "        self.actc = ACTC()  # Assuming ACTC is a defined PyTorch module\n",
    "        self.mdr_inp = nn.Conv2d(64, 3, kernel_size=1) \n",
    "        self.mdsr1 = MDSR1(32)  # Assuming MDSR1 is a defined PyTorch module\n",
    "        self.rdn_inp = nn.Conv2d(64, 128, kernel_size=1) \n",
    "        self.rdn = RDN(128)  # Assuming RDN is a defined PyTorch module\n",
    "        self.conv4 = nn.Conv2d(102, 3, kernel_size=3, padding=2, dilation=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        f1 = self.actc(x)\n",
    "        md_in = self.mdr_inp(x)\n",
    "        f2 = self.mdsr1(md_in)\n",
    "        rd_in = self.rdn_inp(x)\n",
    "        f3 = self.rdn(rd_in)\n",
    "        inp = torch.cat([f1, f2, f3, x], dim=1)\n",
    "        x = self.conv4(inp)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTC module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACTC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ACTC, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(64, 7, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 7, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 7, kernel_size=7, padding=3)\n",
    "        self.conv4 = nn.Conv2d(21, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(x)\n",
    "        x2 = torch.sigmoid(x)\n",
    "        x3 = x2 * x\n",
    "        x4 = F.softplus(x)\n",
    "        x4 = torch.tanh(x4)\n",
    "        x5 = x4 * x\n",
    "        c1 = self.conv1(x1)\n",
    "        c2 = self.conv2(x3)\n",
    "        c3 = self.conv3(x5)\n",
    "        cx = torch.cat([c1, c2, c3], dim=1)\n",
    "        y = self.conv4(cx)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R1 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R1(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(R1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(features, features, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(features, features, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        x = F.relu(self.conv1(input_tensor))\n",
    "        x = self.conv2(x)\n",
    "        return x + input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDSR1 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDSR1(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super(MDSR1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, f, kernel_size=3, padding=1)\n",
    "        self.r1 = R1(f)  # Assuming R1 is a defined PyTorch module\n",
    "        self.conv2 = nn.Conv2d(4*f, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, ix):\n",
    "        x = F.relu(self.conv1(ix))\n",
    "        x1 = self.r1(x)\n",
    "        x1 = self.r1(x1)\n",
    "        x2 = self.r1(x)\n",
    "        x2 = self.r1(x2)\n",
    "        x3 = self.r1(x)\n",
    "        x3 = self.r1(x3)\n",
    "        x = x1 + x2 + x3\n",
    "        x = torch.cat([x, x1, x2, x3], dim=1)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=0, dilation=2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0, dilation=4)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=1, padding=0, dilation=16)\n",
    "        self.den_inp = nn.Conv2d(64, 3, kernel_size=1) \n",
    "        self.den = Den()  # Assuming Den is a defined PyTorch module\n",
    "        self.runt_inp = nn.Conv2d(64, 16, kernel_size=1) \n",
    "        self.runt = Runt(16)  # Assuming Runt is a defined PyTorch module\n",
    "        self.conv4 = nn.Conv2d(6, 3, kernel_size=1, padding=1, dilation=8)  # Adjusted number of input channels\n",
    "        self.runt_inp2 = nn.Conv2d(3, 32, kernel_size=1)\n",
    "        self.runt2 = Runt(32)  # Assuming Runt is a defined PyTorch module\n",
    "        self.conv5 = nn.Conv2d(6, 3, kernel_size=1, padding=1, dilation=8)  # Adjusted number of input channels\n",
    "\n",
    "    def forward(self, input_im):\n",
    "        x = F.relu(self.conv1(input_im))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        den_in = self.den_inp(x) \n",
    "        x1 = self.den(den_in)\n",
    "        runt_in = self.runt_inp(x)\n",
    "        x2 = self.runt(runt_in)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x1 = self.den(x)\n",
    "        runt_in = self.runt_inp2(x)\n",
    "        x2 = self.runt2(runt_in)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        y = self.conv5(x)\n",
    "        # Upsample the output to be 4 times the size of the input\n",
    "        y = F.interpolate(y, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MyModel()\n",
    "\n",
    "# Define a loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = float('inf')\n",
    "best_model_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  \n",
    "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")):\n",
    "        for input1, target in zip(inputs, targets):\n",
    "            if torch.cuda.is_available():\n",
    "                input1 = input1.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            output = model(input1.unsqueeze(0))\n",
    "\n",
    "            loss = criterion(output, target.unsqueeze(0))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f'Training Loss: {loss.item()}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(valid_loader):\n",
    "            for input1, target in zip(inputs, targets):\n",
    "                if torch.cuda.is_available():\n",
    "                    input1 = input1.cuda()\n",
    "                    target = target.cuda()\n",
    "\n",
    "                output = model(input1.unsqueeze(0))\n",
    "\n",
    "                val_loss = criterion(output, target.unsqueeze(0))\n",
    "\n",
    "    print(f'Validation Loss: {val_loss.item()}')\n",
    "    \n",
    "    scheduler.step(val_loss.item())\n",
    "    \n",
    "    if val_loss.item() < min_val_loss:\n",
    "        min_val_loss = val_loss.item()\n",
    "\n",
    "        if not os.path.exists('./weights'):\n",
    "            os.makedirs('./weights')\n",
    "        \n",
    "        # If a better model is found, delete the previous best model\n",
    "        if best_model_path is not None:\n",
    "            os.remove(best_model_path)\n",
    "        \n",
    "        # Save the new best model and update the best model path\n",
    "        best_model_path = f'./weights/model_epoch_{epoch+1}_val_loss_{val_loss.item():.5f}.pth'\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_path = './weights/model_epoch_7_val_loss_0.06323117017745972.pth' \n",
    "weights_path = './weights/model_epoch_5_val_loss_0.07716.pth' \n",
    "\n",
    "# Load the weights into the model\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = input1.unsqueeze(0)\n",
    "# t = t.cuda()\n",
    "# t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_to_image(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = model(t)\n",
    "# o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_to_image(o[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_to_image(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with set5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_image_dir = '/mnt/f/datasets/classical_SR_datasets/Set5/LRbicx4' \n",
    "original_image_dir = '/mnt/f/datasets/classical_SR_datasets/Set5/original' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_image_files = [f for f in os.listdir(lower_image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "original_image_files = [f for f in os.listdir(original_image_dir) if f.endswith('.jpg') or f.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_images = [Image.open(os.path.join(lower_image_dir, f)) for f in lower_image_files]\n",
    "original_images = [Image.open(os.path.join(original_image_dir, f)) for f in original_image_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lower_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(original_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = get_transform()\n",
    "test = trans(lower_images[i])\n",
    "test = test.unsqueeze(0)\n",
    "test = test.cuda()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(test)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
